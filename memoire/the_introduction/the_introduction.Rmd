---
title: "Performance comparison of the discrete choice models of consumer choice"
subtitle: "Exploration of the Econometrics and Machine Learning model performances in the presence of heterogeneous preferences and random effects utilities"
author: |
    | \large Nikita Gusarov
    | \normalsize Master 2
    | MIASHS C2ES (UGA)
    |
    |
    | \hspace{50mm}
    |
    | 
    | \raggedright Under supervision of: 
    | \raggedright\hspace{10mm} Iragaël Joly, HDR (GAEL, UGA, Grenoble INP)
    | \raggedright\hspace{10mm} Beatrice Roussillon, MCF (GAEL, UGA)
    | \hspace{-40mm}
output: 
    pdf_document:
        latex_engine: lualatex
        number_sections: TRUE
        toc: FALSE
        df_print: "kable"
        fig_width: 5
        fig_height: 4
        fig_caption: TRUE
        includes:
            in_header: ../auxilary/packages.sty
# Fonts as recommended in guidelines
# Works only with lualatex engine
# XITS is one of the analogs for proprietary Times New Roman
fontsize: 12pt 
mainfont: XITS 
linestretch: 1.15 # This is standard for Word Documents (MS Word 2007, 2010, 2016)
# Add autogenerated bibentry for r packages
bibliography: 
    - ../../litterature/bibliographie.bib 
    - ../../litterature/packages.bib
    - ../../litterature/supplementary/bibliographie.bib
biblio-style: ../../article/template/ecai2012
nocite: |
    | @R-tinytex, @R-tidyverse, @R-rmarkdown, @R-knitr, @R-stargazer, @R-mlogit
    | @R-foreign, @R-foreach, @R-base, @R-arsenal, @R-keras, @R-tensorflow
    | @R-sdcm
---




 
<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

\begin{description}

\item[Abstract:] This works is a cross-disciplinary study of econometrics and machine learning (ML) models applied to consumer choice modelling. 
To breach the interdisciplinary gap an integrated simulation and theory-testing framework is proposed. 
It incorporates all essential steps from hypothetical setting generation to the comparison of various performance metrics. \vspace{0.1cm}\\ 
The flexibility of the framework in theory-testing and models comparison over economics and statistical indicators is illustrated based on the work of Michaud, Llerena and Joly (2012). 
Two datasets are generated using the predefined utility functions simulating the presence of homogeneous and heterogeneous individual preferences for alternatives' attributes. 
Then, three models issued from econometrics and ML disciplines are estimated and compared. \vspace{0.1cm}\\ 
This study shows the proposed methodological approach's efficiency, successfuly capturing the differences between the models issued from different fields given the homogeneous or heterogeneous consumer preferences. 

\item[Key words:] Consumer Choice, Preference Studies, Willingness to Pay, Econometrics, Data Science, Machine Learning, Classification Techniques, Synthetic Datasets

\item[Author:] Nikita Gusarov (UGA)

\item[Under supervision of:] Iragaël Joly, HDR (GAEL, UGA, Grenoble INP); Beatrice Roussillon, MCF (GAEL, UGA)

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

\item[Abstrait:] Ce travail est une étude interdisciplinaire des modèles d'économétrie et d'apprentissage automatique (ML) appliqués à la modélisation des choix des consommateurs.
Pour briser la frontière interdisciplinaire, un cadre intégré pour tester des théorie est proposé.
Il intègre toutes les étapes essentielles de la génération de paramètres hypothétiques à la comparaison de diverses mesures de performance. \vspace{0.1cm}\\ 
La flexibilité du cadre dans les tests de théorie et la comparaison de modèles par rapport aux indicateurs économiques et statistiques est illustrée à partir des travaux de Michaud, Llerena et Joly (2012).
Deux ensembles de données sont générés à l'aide des fonctions d'utilité prédéfinies simulant la présence de préférences individuelles homogènes et hétérogènes pour les attributs des alternatives.
Trois modèles issus des disciplines économétrie et ML sont ensuite estimés et comparés. \vspace{0.1cm}\\ 
Cette étude montre l'efficacité de l'approche méthodologique proposée, en captant avec succès les différences entre les modèles issus de différents domaines compte tenu des préférences homogènes ou hétérogènes des consommateurs.

\item[Mots clés:] Choix du consommateur, \'Etudes de Préférences, Consentements à Payer, \'Econométrie, Science des Données, Apprentissage Automatique, Techniques de Classification, Données Synthétiques

\end{description}

\newpage




 
<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

\begin{center}
\textbf{\Large Acknowledgements}
\end{center}
\vspace{2.3ex}

This work was accomplished with financial aid from Multidisciplinary Institute in Artificial Intelligence (MIAI), supported by Sihem Amer-Yahia, head of the SLIDE team at the LIG laboratory. 

\vspace{10mm}
I would like to express my gratitude for the administrative and technical support from the Grenoble Informatics Laboratory (LIG) and Grenoble Applied Economics Laboratory (GAEL), which helped to fulfil this work during COVID-19 crisis.

\vspace{10mm}
Credits for dataset generation algorithm go to Amirreza Talebijamalabad, first year master student at Grenoble INP, who worked on the theory of the artificial datasets generation.


\newpage





<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

<!-- Table of contents -->
\pagenumbering{roman}

\setcounter{tocdepth}{1}
<!-- \tableofcontents -->
\shorttoc{Summary}{2}

\newpage

<!-- Switch for main body page numbering -->
\pagenumbering{arabic}







<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

```{r Knitr, echo = FALSE}
#################
# Knitr options #
#################

knitr::opts_chunk$set(
    fig.align = "center",
    # eval = FALSE,
    echo = FALSE
)
```

```{r Packages, echo= FALSE, include = FALSE}
####################
# PAckages and AUX #
####################

# General document wide packages
library(foreign)
library(arsenal)
library(stargazer)
library(keras)
library(tensorflow)
# library(png)
library(sdcm)

# Arsenal transformation
source("../../code/arsenal_to_stargazer.R")
source("../../code/amirreza_functions.R")
# transform_arsenal(
#     input = ,
#     position = "!htbp",
#     caption = "Table",
#     label = ""
# )

# Override by loading last
library(tidyverse)

# Add used packages to biliography
knitr::write_bib(c(.packages(), "rmarkdown", "knitr", "tinytex"), "../../litterature/packages.bib")
```




 
<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

# Introduction {-}

The advances in statistical learning, data analysis and data science of the past decades  have resulted in propagation of *Machine Learning* (ML) techniques to different applied fields, including social and human sciences. 
Nowadays, it is impossible to imagine a field of science that is not benefiting from the fruits of statistical learning.
The works of @depalma2011tr and @cascetta2009tr on transportation modelling, the publications of @molina2019soc dedicated to sociology problematic, the articles of @coussement2010gam concerning marketing decisions, actuary analysis studies (@denuit2019as1, @denuit2019as3) or even psychology with an example of @baayen2017gam work reflect the literal omnipresence of the newly developed techniques.

However, there exist two completely distinct approaches to applying statistical learning, as described by @breiman2001stat and latter by @athey2019ml: the *Machine Learning* which focuses on the predictive qualities (figure \ref{fig:parad3}) and *Econometrics* which attempts to decipher the underlying properties of the data (figure \ref{fig:parad2}).
In economics, where the research is focused on hidden patterns exploration, the scientific community prefers to implement the traditional econometrics techniques using the more advanced statistical models only in some special cases or as some assistance tools [@athey2018iml]. 
This discrepancy is explained by the fact that econometrics, contrary to traditional ML paradigm focusses on the accessibility of results.
Consequently, many of the advanced ML techniques rarely appear in economics publications because of their believed lack of interpretability and excessive complexity in application.
Nevertheless, some multidisciplinary scientists make attempts to breach this wall between *ML* and *Econometrics*: @varian2014bd, @mullainathan2017ml or, among the most recent, @athey2019ml.
Their advances are mostly focused on resolving the general interdisciplinary tool-set integration questions, without considering the application specific details. 
Nevertheless, in the attempt to breach the interdisciplinary barrier the details reveal themselves to be of utmost importance in the solution of the problem. 

\begin{figure}[hbtp]
\centering
\caption{The different paradigms}
\label{fig:parad}
\begin{subfigure}[c]{.4\linewidth}
    \centering
    \caption{Real world}
    \label{fig:parad1}
    \begin{tikzpicture}[box/.style = {draw, text width=2cm, align=center}]
        \node[box] (b) {Nature};
        \node[left=of b] (a) {$\mathcal{X}$};
        \node[right=of b] (c) {$\mathcal{Y}$};
        \draw[->] (a) -- (b);
        \draw[->] (b) -- (c);
    \end{tikzpicture}
\end{subfigure}\hspace{12pt}\vspace{12pt}

\begin{subfigure}[c]{.4\linewidth}
    \centering
    \caption{Econometrics}
    \label{fig:parad2}
    \begin{tikzpicture}[box/.style = {draw, text width=2cm, align=center}]
        \node[box] (b) {Theoretical\\model};
        \node[left=of b] (a) {$X$};
        \node[right=of b] (c) {$Y$};
        \draw[->] (a) -- (b);
        \draw[->] (b) -- (c);
    \end{tikzpicture}
\end{subfigure}\hspace{12pt}
\begin{subfigure}[c]{.4\linewidth}
    \centering
    \caption{Machine Learning}
    \label{fig:parad3}
    \begin{tikzpicture}[box/.style = {draw, text width=2cm, align=center}]
        \node[box] (b) {Nature};
        \node[box, below=of b] (d) {ML\\model};
        \node[left=of b] (a) {$X$};
        \node[right=of b] (c) {$Y$};
        \draw[->] (a) -- (b);
        \draw[->] (b) -- (c);
        \draw[->] (a) |- (d);
        \draw[->] (d) -| (c);
    \end{tikzpicture}
\end{subfigure}\hspace{12pt}
\end{figure}

There have already been a multitude of studies comparing the performances of different econometric and ML models in various real world scenarios: the study of machine learning methods to model the car ownership demand estimation of @paredes2017machine, for example; or the use of decision trees in microeconomics of @brathwaite2017machine.
However, there's no known to us work incorporating at least all the baseline models, as it would require an unimaginable amount of efforts to accomplish.
For instance, in the literature the performance of competing models are studied according to several absolutely alien criteria: in terms of the quality of data adjustments, in terms of predictive capacity, as well as in terms of the quality of the economic and behavioural indicators derived from estimates and, finally, according to their algorithmic efficiency and computational costs.
None of the known to us articles manages to incorporate all these aspects into their benchmarks, limiting their studies only with several performance criteria. 

These various aspects, greatly impact the performance of particular models or algorithms, although some of them are often ignored by the researchers.
Not only there exists inconsistency in the targeted performance metrics in the contemporary models' comparisons, but there is also omnipresent problems of theoretical background choice, dataset selection or model's specifications.
For example, speaking about the datasets used to support their findings, many researchers explore the impacts of different specifications on the same observed or simulated choice situation (@munizaga2005mlyp, @fiebig2010gmlm, @mccausland2013pd, @joly2019qcm) as it appears to be the most theoretically reliable procedure.
However, there is still no established unified methodology documenting this field.

<!-- preliminary questions in the model building phase greatly impact the performance of particular models or algorithms, although some of them are often ignored by the researchers -->

<!-- - le paragraphe donne l'impression que le sujet du stage vient de la volonté de réduire le sujet... Il serait préférable de dire qu'il contribue au tableau général que vous venez de donner.
- il faudrait donc lister les arguments rendant le focus sur les DCM intéressants (voire le plus intéressant) : je vois au moins
+ Choice modelling relates to DCM in econometrics (one of the major development associated with McFadden nobel prize) and their counterpart in ML: classification tools (SVM, NN, etc)
+ DMC are not only focussing on prediction of choice (market shares), or correlation analysis (to identify impacting variables) but also used to deduce elasticities and WTP: specifying individual expected utilities and preferences.
+ Comparing results of DCM and ML techniques on categorical variables is a well documented topics, with well-known indicator (confusion matrix, etc) -->

<!-- Au vu de sa position dans l'introduction il faudrait qu'il énonce un objectif général - objectif qui sera précisé petit à petit avec les sous sections qui suivent.
=> il faudrait essayer de conclure chaque sous section par une phrase qui précise l'objectif ou ses enjeux. Par ex pour les données, il faudrait terminer par "donc on va simuler des données". Ce qui signifie aussi qu'il faut argumenter sur le choix qui est fait (ces arguments sont peut être plus loin dans la partie I ?)Globalement les sous section ne vont pas jusqu'à définir ou borner ou justifier ce qui sera fait.
Ainsi l'introduction va définir la problématique pas à pas en la positionnant à la littérature existante et ses débats
Un début de résumer:
1) Consumer choice: de multiple modèle économétriques existent associés à des théories comportementales.
=> nous nous restreignons ici aux RUM avec préférences hétérogènes
=> d'ailleurs il faut faire référence à ces random utilities et préf hétérogènes dans ce paragraphes
2) math modèle: Des situations de choix diverses et potentiellement complexes associés à des outils pouvant être très affinés et calibrés pour ces situations.
=> nous étudierons les situations les plus simples: celles du MNL, qui nous permettra de mobiliser les outils de ML et aussi d'approfondir avec random utilities
3) Data
L'étude des comportements des modèles et leurs perf se fait sur données réelles ou simulées
=> Nous voulons évaluer la performance relative des outils de DS dans une situation de choix, tout en contrôlant tous les paramètres. Nous simulons les données pour contrôler le cadre expérimental (ou chaque modèle est un sujet observé face à différents traitements) -->

<!-- New comments -->

<!-- Il manque un paragraphe juste avant qui définisse de quelle étude vous parlez.
C'est aussi le moment (1ère partie de l'intro) pour définir / expliquer le titre du mémoire
Je pense que c'est la place pour définir les grandes lignes de votre travail: évaluer la performance de modèles concurrents. 
Comme vous venez de dire que c'était beaucoup de modèle, votre travail propose un focus sur les modèles DCM car c'est un outil central en économie
C'est donc aussi la place pour "vendre" le sujet aux économistes en donnant les enjeux pour l'économie du travail proposé:
- test de modèles comportementaux différents (d'où la sous section sur les comprtement juste après)
- test de modèles quanti concurrents potentieillement sensibles aux hypothèses ou dépendants de leurs hypothèses (d'où la sous section)
- mise en place d'une méthodologie d'évaluation de perf alliant reproductibilité et contrôle des conditions expérimentales (d'où data et context) ("context" pourrait devenir "method"?) -->

From this unambiguity in the scientific community the main problematic of this work arises. 
It is particularly important to establish a common framework for performance comparison of the discrete choice models be they from the econometrics or ML tool-set.
However, this task cannot be accomplished outside a precise context, which will potentially impose some limitations over the models' structure, as well as influence the choice of performance metrics. 
In economics the discrete choice models are extensively used for consumer choice analysis [@anderson1992discrete], willingness to pay derivation [@llerena2013rose] and other preference studies. 
The field specific theories and traditional research objectives frame and define this study's scope.

<!-- Exploration of the Econometrics and Machine Learning model performances in the presence of heterogenous preferences and random effects utilities ... -->

From the economics perspective there exist three major points of interest to be taken into account.
First, there is a strong interest in economics to explore the different behavioural set-ups, under different settings and assumptions.
Secondly, given the different choice situations there is a potential need to test how the available mathematical models, potentially sensitive to the tested behavioural hypotheses or dependent on these hypotheses, perform in a given context.
Last, but not least, a comprehensive implementation of a performance evaluation methodology, combining reproducibility and control of experimental conditions, should be introduced in the proposed framework. 






<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

**Consumer choice**

The economic decision theory derives mostly from the random utility theory (RUM) of @mcfadden1974utd and more recently of @mcfadden2001ec, that were recently challenged by alternative visions such as random regret minimisation theory (RRM) of @chorus2010rrm, with a related relative advantage maximisation theory (RAM) of @leong2015ram, or even quantum decision theory (QDT) of @yukalov2017quantum, which offers a wide range of tools for modelling under uncertainty.

These different theories address various aspects of the decision making process, under different suppositions and incorporating different biases.
For example, one of the basic assumptions of the traditional choice theory is the transitivity of choice, meaning there exists a strict hierarchy of individual preferences among alternatives. 
This assumption may be unsuitable for real world choice situation and lead to potential bias, which is addressed by quantum decision theory. 
QDT manages to bypass this shortcoming and incorporate non-transitivity of choices into the framework.
There exist a multitude of other behavioural elements unexplained by the most traditional models that may be incorporated into the decision making framework, such as loss aversion for example, that could be addressed with random regret minimisation theory.

There is a particular interest in detecting the differences in the models' performances depending on the choice context and the assumed decision-making framework. 
It is important, because different consumer behaviour in the individual choice context result in different choice distributions, which may affect the models' performances. 
In economics RUM theory is nowadays one of the most used choice settings in the individual decision modelling. 
Nevertheless, there still exist some unexplored limitations, that such theoretical framework may impose over the estimation techniques, as well as to what potential biases a model's misspecification may lead. 





<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

**Mathematical models**

In general any classification technique may be used to model individual decisions, although nearly every model has some restrictions and limitations, which may largely affect its performances in a given context. 

Usually the choice of model is rarely discussed in applied studies, as the researchers tend to use either the simplest model possible or attempt to implement one particular model of interest ignoring some times the other possible choices. 
For example, many traditional econometrics studies, given a multiple choice problem context, use a multinomial logistic regression (MNL) or even simplify the problem to a binary case, allowing to implement even more traditional models such as binary logit or binary probit models. 
However, there exists a multitude of particular cases in modelling individual choices, that require specific techniques to be implemented.
A family of duration models may be used to model the individual decisions over time (@vitetta2016quantum); network modelling that allows to incorporate spatial and social dependencies for the explored data (@brock2003mcsi); preference learning techniques aiming to explore the positioning of different alternatives by an individual (@tsoukias2013ph, @pigozzi2016pai) and many other advanced techniques from *machine learning* field such as neural networks or support vector machines.

An incorrect choice of the modelling technique may have a strong impact on the derived target values leading to some erroneous conclusions in the end. 
For example, an incorrectly estimated willingness to pay for a particular product may lead to significant losses.
When conducting an applied research study one should always be conscious of the eventual biases introduced by the choice of the model and the eventual consequences of these choices. 
Some models are not suitable to be implemented on a particular set of data, while others are unable to provide necessary information about the relationships within a particular dataset or derive the particular target values of interest. 

Taking into account the implications of RUM theory, there exists a particular interest to make the focus on the state of art econometric discrete choice models (@agresti2013cd, @agresti2007cd, @baltagi2008econometric, @train2009dc, @mcfadden2001ec, @mcfadden1974utd) as well as their counterparts used in ML (@hastie2009sl, @kotsiantis2006tr). 
A comparison of some simple models against more complex ones may reveal the trade-off between precise estimates and the resources invested.

<!-- New comment -->

<!-- Je remettrais aussi un petit paragraphe avant cette conclusion pour rappeler qque RUM => WTP, qui sont déduites des estimations, dont on évalue peu la sensibilité à la méthode d'estimation -->

<!-- il serait préférable
- qu'il soit plus court,
- il pourrait être organiser pour dire que le chercheur doit faire un choix entre la simplicité et la complexité donc la réduction du modèle et la précision du modèle. -->





<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

**Data**

Different sources of data are available for a researcher, that could be divided into two groups [@japkowicz2011el]: 
*field datasets*, which are gathered through an experiment or collected from the real world observations or real world uncontrolled experiment; 
and *synthetic datasets*, which are artificially generated by the researcher to suit his needs and respect some particular limitations. 
Although this variability of dataset choices not that evident in the context of applied studies, there is an ongoing debate concerning the eventual impacts of data choice on the models' performances and resulting metrics.
<!-- "the eventual impacts of data choice on the models’ performances and resulting metrics make dataset choices a sensitive question (ça doit pas être correct ? - peut être corner stone) in the context of applied studies" -->

Given a task of performance evaluation and comparison for different algorithms or mathematical models there is always a difficult choice of the data type to be used in the study. 
Both of the mentioned above dataset types have their advantages and disadvantages and require a particular attention.
However, having for objective the theory- and model-testing framework construction there is a strong interest to use the artificially generated data in order to have as much control as possible over the situation.

<!-- Inthis work to demonstate all the power and flexibility of the deviced testing framework the choice is made to  -->





<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

<!-- ##### Performance metrics {-} -->





<!-- ################################################################## -->
<!-- ################################################################## -->
<!-- ################################################################## -->

**The framework and context**

<!-- Faire précéder cela d'un énoncé de la problématique complète après les sous sections -->

Given these three key elements we propose an integrated simulation and theory-testing framework which will encompass all the different aspects of the model comparison task.
The steps to be integrated into such framework encompass many theoretical questions starting with the underlying theoretical assumptions and ending with the choice of correct performance metrics. 
Consequently, this work attempts to fill the gap between two statistical paradigms: *econometrics* and *machine learning*, taking into account the key elements, among which the different combinations of decision theory assumptions, dataset generation procedures, mathematical models and target performance measures. 
The problematic arises from the insufficient points of contact among researchers from different fields of applications, as well as insufficiently unified methodology to put into relations the different approaches. 
A work that uses unified knowledge from several disciplines might be highly beneficial for the scientific community as it will lie a foundation and provide support for future applied studies.
Following the logic of @athey2018iml and @mullainathan2017ml the project will attempt to merge the essentials of ML and econometrics paradigms, retaining their key concepts in the context of consumer choice problem.

<!-- je parlerais ici de mise en contexte, plutôt que point de départ.

Il faudrait faire précéder ce paragraphe
- d'une phrase ou paragraphe qui annonce le plan de la partie théorique de design de toute la chaîne
- d'une phrase sur la partie empirique du travail, qui doit résoudre la question du contexte empirique de l'étude: cela répond à la question : quelle situation de choix doit on étudier ? quels consommateurs et quel est leur comportement? -->

<!-- New comments  -->

<!-- Cette version est bien, je ne sais pas s'il est nécessaire / possible de la changer 
Je l'aurais présentée dans l'ordre inverse, en disant partant de 
ce qu'il faut pour faire votre étude (une sorte de cahier des charges) et dire que prendre un papier appliqué est la bonne stratégie (surtout si vous travailler avec ses auteurs)
pour faire ce travail d'évaluation en allant jusqu'aux usages classiques des économistes, un cadre connu ou usuel est nécessaire, notamment 1) pour illustrer les usages en économie des modèles que l'on discute 2) donner un aperçu de l'intérêt de votre étude pour les économistes: comment intégrer vos questionnments dans l'eur pratique,etc.
L'essentiel des justifications sont déjà présentent dans votre paragraphe, ce n'est que la façon de l'amener qui diffère -->

<!-- what it takes to do your study (a kind of specification) and say that taking an applied paper is the right strategy (especially if you are working with its authors)
to do this evaluation work by going as far as the classic uses of economists, a known or usual framework is necessary, in particular 1) to illustrate the uses in economics of the models that we are discussing 2) to give an overview of the interest of your study for economists: how to integrate your questions into the practical euro, etc. -->

<!-- To accomplish the defined objective it is a good strategy to choose an existing economic work, which uses the discrete choice models as a tool in an empirical study.  -->
We propose to use an applied paper in econometrics of choice modelling to facilitate understanding of the field of application and tools.
This means not that we will attempt to replicate the results, but rather to use the context provided in the work for demonstration of the proposed hypothesis-testing framework.
We select the article of @llerena2013rose as our reference paper, because of the advantages to work directly with the authors of the paper.  <!-- make more clear -->
The work of @llerena2013rose is focused on investigation of consumers’ willingness to pay (WTP) for environmental attributes of a non-food agricultural products, taking roses as example. 
Authors constructed an experimental framework to derive the premium the testing subjects were ready to pay for such environmental attributes as lower carbon imprint and ecological labelling, certifying the source of the environmentally friendly practices.
That study explored individual preferences for roses with an eco-label and a carbon footprint using discrete choice modelling techniques and real economic incentives resulting in real purchases of roses.
The gathered dataset was analysed with a mixed logit model demonstrating notorious premiums for both attributes. 
We will benefit of the obtained results to demonstrate all of the complexity of a proposed theory-testing framework, its functionality and perspectives. 

The present report is divided into two main parts. 
The first section presents the chosen context for this work followed by short presentations of all the theoretical aspects which play their major roles in this study, tracing at the same time parallels with the context. 
The second part presents the results of all the results step-by-step, demonstrating the functionality of the designed framework.
Each of the sections has an identical logical structure of presentation of the framework's components in the successive order: starting with the behavioural modelling and data related questions, directly followed by the models' presentation and the performance measures.
The final section concludes.

<!-- parfait pour une conclusion ou un abstract (après remaniement) mais n'a pas sa place en fin d'introduction il faut finir le paragraphe précédant par une annonce de plan de la partie empirique -->

<!-- To sum up, this work contributes to the interdisciplinary unification of very different domains in the context of decision making modelling.
A detailed study will allow in future to propose a flexible testing tool adapted for hypothesis testing and selection of an appropriate method for a specific application in relation with cognitive and behavioural studies.
The valuable insights into model performance in the context of discrete individual choice, as well as underlying behavioural theories influences on the results are expected to be obtained.  -->

\newpage
