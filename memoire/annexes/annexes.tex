\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{placeins}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{arrows.meta}
\usepackage{amsmath}
\usepackage[edges]{forest}
\usepackage{multicol}

\begin{document}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\newpage

\hypertarget{appendices}{%
\section*{Appendices}\label{appendices}}
\addcontentsline{toc}{section}{Appendices}

\hypertarget{a-taxonomies-of-statistical-models}{%
\subsection*{A Taxonomies of statistical
models}\label{a-taxonomies-of-statistical-models}}
\addcontentsline{toc}{subsection}{A Taxonomies of statistical models}

\FloatBarrier

\begin{figure}[!htbp]
\centering
\caption{Taxonomy as proposed by Hastie and Tibshirani (2009), reduced form}
\label{fig:ayod}
\begin{forest}
  for tree={
    align=center,
    edge+={ -{Stealth[]}},
    l sep'+=10pt,
    fork sep'=10pt,
  },
  forked edges,
  if level=0{
    inner xsep=0pt,
    tikz={\draw (.children first) -- (.children last);}
  }{},
  [Machine Learning 
    [Supervised\\learning
        [Non-linear\\methods
            [Additive\\models]
            [SVM]
            [Prototype\\methods]
            [Neural\\Networks]
        ]
        [Linear\\methods
            [Classification
                [Discriminant\\analysis]
                [Logistic\\regression\\and\\generalisations]
                [Separating\\hyperplanes]
                [Identity\\regression]
            ]
            [Regression]
        ]
    ]
    [Unsupervised\\learning]
  ]
\end{forest}
\end{figure}

\begin{figure}[!htbp]
\centering
\caption{Taxonomy as proposed by Ayodele (2010)}
\label{fig:hast}
\begin{forest}
  for tree={
    align=center,
    edge+={ -{Stealth[]}},
    l sep'+=10pt,
    fork sep'=10pt,
  },
  forked edges,
  if level=0{
    inner xsep=0pt,
    tikz={\draw (.children first) -- (.children last);}
  }{},
  [Machine Learning 
    [Semi-supervised\\learning]
    [Learning\\to\\learn]
    [Reinforcement\\learning]
    [Supervised\\Learning
        [Decision\\trees]
        [Neural\\networks]
        [Bayesian\\networks]
        [K-means\\clustering]
        [Linear\\classifier]
        [Quadratic\\classifier]
    ]
    [Unsupervised\\learning]
    [Transduction]
  ]
\end{forest}
\end{figure}

\begin{figure}[!htbp]
\centering
\caption{Taxonomy as proposed by Agresti (2013), based on data types}
\label{fig:agre}
\begin{forest}
  for tree={
    align=center,
    edge+={ -{Stealth[]}},
    l sep'+=10pt,
    fork sep'=10pt,
  },
  forked edges,
  if level=0{
    inner xsep=0pt,
    tikz={\draw (.children first) -- (.children last);}
  }{},
  [Machine Learning 
    [GLM
        [Multinomial\\responses
            [Ordinal\\responses
                [Cumulative\\logit]
            ]
            [Nominal\\responses
                [Baseline-\\Category\\logit]
            ]
        ]
        [Binary\\responses
            [Conditional\\logit]
            [Probit]
            [Logit]
            [Complementary\\Log-log\\models]
        ]
    ]
  ]
\end{forest}
\end{figure}

\FloatBarrier

\clearpage

\hypertarget{b-performance-measures-positioning}{%
\subsection*{B Performance measures
positioning}\label{b-performance-measures-positioning}}
\addcontentsline{toc}{subsection}{B Performance measures positioning}

\FloatBarrier

\begin{figure}[!htbp]
\centering
\caption{Performance measures as described by Japkowicz (2011)}
\label{fig:japk}
\begin{forest}
  for tree={
    align=center,
    edge+={ -{Stealth[]}},
    l sep'+=10pt,
    fork sep'=10pt,
  },
  forked edges,
  if level=0{
    inner xsep=0pt,
    tikz={\draw (.children first) -- (.children last);}
  }{},
  [All measures 
    [Confusion\\matrix\\based
        [Deterministic\\classifiers
            [Multiclass\\focused
                [Accuracy\\Error rate]
            ]
            [Single-class\\focused
                [TP-TN ratios\\F-measure\\Geometric means\\...]
            ]
        ]
    ]
    [Additional\\information
        [Scoring\\classifiers
            [Graphical\\measures
                [ROC\\PR\\DET\\Lift\\and Cost curves]
            ]
            [Summary\\statistics
                [AUC]
            ]
        ]
        [Continuous\\Probabilistic\\classifiers
            [Distance\\error\\measures
                [RMSE]
            ]
            [Information\\Theoretic\\measures
                [KLD\\KB IR\\BIR]
            ]
        ]
    ]
    [Alternative\\information
        [Multicriteria\\measures]
    ]
  ]
\end{forest}
\end{figure}

\FloatBarrier

\newpage

\FloatBarrier

\hypertarget{c-descriptive-statistics}{%
\subsection*{C Descriptive statistics}\label{c-descriptive-statistics}}
\addcontentsline{toc}{subsection}{C Descriptive statistics}

\hypertarget{c.1-comparing-datasets-over-a-alternative}{%
\subsubsection*{C.1 Comparing datasets over A
alternative}\label{c.1-comparing-datasets-over-a-alternative}}
\addcontentsline{toc}{subsubsection}{C.1 Comparing datasets over A
alternative}

\FloatBarrier

\FloatBarrier

\begin{table}[!htbp] \centering 
  \caption{Alternatives' descriptive statistics by dataset, stratified by alternative} 
  \label{tab:stratA} 
\begin{tabular}{@{\extracolsep{5pt}}llcccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Alternative &  & Fixed Effects  & Random Effects  & Target  & p value\\
 & & (N=320000) & (N=320000) & (N=2372) &  \\
\hline \\[-1.8ex] 
A & \textbf{Alternative} &  &  &  & \\
 & ~~~A & 160000 (100.0\%) & 160000 (100.0\%) & 1186 (100.0\%) & \\
 & ~~~B & 0 (0.0\%) & 0 (0.0\%) & 0 (0.0\%) & \\
 & \textbf{Choice} &  &  &  & < 0.001\\
 & ~~~Mean (SD) & 0.427 (0.495) & 0.382 (0.486) & 0.517 (0.500) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Price} &  &  &  & 0.022\\
 & ~~~Mean (SD) & 3.069 (0.979) & 3.069 (0.979) & 2.990 (0.881) & \\
 & ~~~Range & 1.500 - 4.500 & 1.500 - 4.500 & 1.500 - 4.500 & \\
 & \textbf{Carbon} &  &  &  & < 0.001\\
 & ~~~Mean (SD) & 0.500 (0.500) & 0.500 (0.500) & 0.167 (0.373) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Label} &  &  &  & 0.993\\
 & ~~~Mean (SD) & 0.500 (0.500) & 0.500 (0.500) & 0.502 (0.500) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Price by group} &  &  &  & < 0.001\\
 & ~~~1.5 & 16000 (10.0\%) & 16000 (10.0\%) & 82 (6.9\%) & \\
 & ~~~2 & 24000 (15.0\%) & 24000 (15.0\%) & 223 (18.8\%) & \\
 & ~~~2.5 & 27000 (16.9\%) & 27000 (16.9\%) & 214 (18.0\%) & \\
 & ~~~3 & 23000 (14.4\%) & 23000 (14.4\%) & 175 (14.8\%) & \\
 & ~~~3.5 & 22000 (13.8\%) & 22000 (13.8\%) & 187 (15.8\%) & \\
 & ~~~4 & 21000 (13.1\%) & 21000 (13.1\%) & 219 (18.5\%) & \\
 & ~~~4.5 & 27000 (16.9\%) & 27000 (16.9\%) & 86 (7.3\%) & \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\newpage

\hypertarget{c.2-comparing-datasets-over-b-alternative}{%
\subsubsection*{C.2 Comparing datasets over B
alternative}\label{c.2-comparing-datasets-over-b-alternative}}
\addcontentsline{toc}{subsubsection}{C.2 Comparing datasets over B
alternative}

\FloatBarrier

\begin{table}[!htbp] \centering 
  \caption{Alternatives' descriptive statistics by dataset, stratified by alternative} 
  \label{tab:stratB} 
\begin{tabular}{@{\extracolsep{5pt}}llcccc}
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Alternative &  & Fixed Effects  & Random Effects  & Target  & p value\\
 & & (N=320000) & (N=320000) & (N=2372) &  \\
\hline \\[-1.8ex] 
B & \textbf{Alternative} &  &  &  & \\
 & ~~~A & 0 (0.0\%) & 0 (0.0\%) & 0 (0.0\%) & \\
 & ~~~B & 160000 (100.0\%) & 160000 (100.0\%) & 1186 (100.0\%) & \\
 & \textbf{Choice} &  &  &  & < 0.001\\
 & ~~~Mean (SD) & 0.518 (0.500) & 0.462 (0.499) & 0.159 (0.366) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Price} &  &  &  & < 0.001\\
 & ~~~Mean (SD) & 2.803 (0.917) & 2.803 (0.917) & 3.020 (0.893) & \\
 & ~~~Range & 1.500 - 4.500 & 1.500 - 4.500 & 1.500 - 4.500 & \\
 & \textbf{Carbon} &  &  &  & < 0.001\\
 & ~~~Mean (SD) & 0.500 (0.500) & 0.500 (0.500) & 0.832 (0.374) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Label} &  &  &  & 0.985\\
 & ~~~Mean (SD) & 0.500 (0.500) & 0.500 (0.500) & 0.497 (0.500) & \\
 & ~~~Range & 0.000 - 1.000 & 0.000 - 1.000 & 0.000 - 1.000 & \\
 & \textbf{Price by group} &  &  &  & < 0.001\\
 & ~~~1.5 & 25000 (15.6\%) & 25000 (15.6\%) & 108 (9.1\%) & \\
 & ~~~2 & 28000 (17.5\%) & 28000 (17.5\%) & 192 (16.2\%) & \\
 & ~~~2.5 & 26000 (16.2\%) & 26000 (16.2\%) & 158 (13.3\%) & \\
 & ~~~3 & 30000 (18.8\%) & 30000 (18.8\%) & 204 (17.2\%) & \\
 & ~~~3.5 & 18000 (11.2\%) & 18000 (11.2\%) & 232 (19.6\%) & \\
 & ~~~4 & 23000 (14.4\%) & 23000 (14.4\%) & 195 (16.4\%) & \\
 & ~~~4.5 & 10000 (6.2\%) & 10000 (6.2\%) & 97 (8.2\%) & \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\newpage

\hypertarget{d-r-code-for-implemented-models}{%
\subsection*{\texorpdfstring{D \emph{R} code for implemented
models}{D R code for implemented models}}\label{d-r-code-for-implemented-models}}
\addcontentsline{toc}{subsection}{D \emph{R} code for implemented
models}

\hypertarget{d.1-mnl-model}{%
\subsubsection*{D.1 MNL model}\label{d.1-mnl-model}}
\addcontentsline{toc}{subsubsection}{D.1 MNL model}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Transform dataset to mlogit format}
\NormalTok{mnl_data =}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mlogit.data}\NormalTok{(}
        \DataTypeTok{choice =} \StringTok{"Choice"}\NormalTok{,}
        \DataTypeTok{alt.var =} \StringTok{"Alternative"}\NormalTok{, }
        \DataTypeTok{shape =} \StringTok{"long"}\NormalTok{, }\CommentTok{# Long format}
        \DataTypeTok{alt.levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"C"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{) }\CommentTok{# Define order of alternatives}
\NormalTok{    )}

\CommentTok{# Function}
\NormalTok{utility =}\StringTok{ }\NormalTok{Choice }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Age }\OperatorTok{+}\StringTok{ }\NormalTok{Salary }\OperatorTok{+}\StringTok{ }\NormalTok{Habit }\OperatorTok{+}\StringTok{ }\CommentTok{# Individual characteristics}
\StringTok{    }\NormalTok{Price }\OperatorTok{+}\StringTok{ }\NormalTok{Buy }\OperatorTok{+}\StringTok{ }\NormalTok{Label }\OperatorTok{+}\StringTok{ }\NormalTok{Carbon }\OperatorTok{+}\StringTok{ }\NormalTok{LC }\OperatorTok{+}\StringTok{ }\DecValTok{0} \OperatorTok{|}\StringTok{ }\DecValTok{0} \CommentTok{# Alternatives attributes}

\CommentTok{# Estimate MNL model}
\NormalTok{mnl_novar =}\StringTok{ }\KeywordTok{mlogit}\NormalTok{(}
\NormalTok{        utility,}
        \DataTypeTok{data =}\NormalTok{ mnl_data, }
        \DataTypeTok{reflevel =} \StringTok{"C"}\NormalTok{, }\CommentTok{# The No-buy option is the baseline}
        \DataTypeTok{print.level =} \DecValTok{3}\NormalTok{, }\CommentTok{# Print estimation details}
        \DataTypeTok{iterlim =} \DecValTok{1000}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{d.2-mmnl-model}{%
\subsubsection*{D.2 MMNL model}\label{d.2-mmnl-model}}
\addcontentsline{toc}{subsubsection}{D.2 MMNL model}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Transform dataset to mlogit format}
\NormalTok{mmnl_data =}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mlogit.data}\NormalTok{(}
        \DataTypeTok{choice =} \StringTok{"Choice"}\NormalTok{,}
        \DataTypeTok{alt.var =} \StringTok{"Alternative"}\NormalTok{,}
        \DataTypeTok{id =} \StringTok{"ID"}\NormalTok{, }\CommentTok{# Set individuals' index}
        \DataTypeTok{chid =} \StringTok{"CHID"}\NormalTok{, }\CommentTok{# Set choice sets index}
        \DataTypeTok{shape =} \StringTok{"long"}\NormalTok{,}
        \DataTypeTok{alt.levels =} \KeywordTok{c}\NormalTok{(}\StringTok{"C"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{)}
\NormalTok{    )}

\CommentTok{# Function}
\NormalTok{utility =}\StringTok{ }\NormalTok{Choice }\OperatorTok{~}\StringTok{ }\NormalTok{Sex }\OperatorTok{+}\StringTok{ }\NormalTok{Age }\OperatorTok{+}\StringTok{ }\NormalTok{Salary }\OperatorTok{+}\StringTok{ }\NormalTok{Habit }\OperatorTok{+}\StringTok{ }\CommentTok{# Individual characteristics}
\StringTok{    }\NormalTok{Price }\OperatorTok{+}\StringTok{ }\NormalTok{Buy }\OperatorTok{+}\StringTok{ }\NormalTok{Label }\OperatorTok{+}\StringTok{ }\NormalTok{Carbon }\OperatorTok{+}\StringTok{ }\NormalTok{LC }\OperatorTok{+}\StringTok{ }\DecValTok{0} \OperatorTok{|}\StringTok{ }\DecValTok{0} \CommentTok{# Alternatives attributes}

\CommentTok{# Estimate MMNL model}
\NormalTok{mmnl =}\StringTok{ }\KeywordTok{mlogit}\NormalTok{(}
\NormalTok{        utility,}
        \DataTypeTok{data =}\NormalTok{ mmnl_data, }
        \DataTypeTok{reflevel =} \StringTok{"C"}\NormalTok{, }\CommentTok{# The No-buy option is the baseline}
        \DataTypeTok{correlation =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Include covariance (and not variance only)}
        \DataTypeTok{rpar =}  \KeywordTok{c}\NormalTok{( }\CommentTok{# Normality assumption and four parameters}
            \StringTok{"Buy"}\NormalTok{ =}\StringTok{ "n"}\NormalTok{, }
            \StringTok{"Label"}\NormalTok{ =}\StringTok{ "n"}\NormalTok{, }
            \StringTok{"Carbon"}\NormalTok{ =}\StringTok{ "n"}\NormalTok{, }
            \StringTok{"LC"}\NormalTok{ =}\StringTok{ "n"}
\NormalTok{        ),}
        \DataTypeTok{panel =} \OtherTok{TRUE}\NormalTok{, }\CommentTok{# Estimate dataset as panel}
        \DataTypeTok{print.level =} \DecValTok{3}\NormalTok{, }\CommentTok{# Print estimation details}
        \DataTypeTok{iterlim =} \DecValTok{1000}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\hypertarget{d.3-cnn-model-with-adam-algorithm}{%
\subsubsection*{\texorpdfstring{D.3 CNN model with \emph{Adam}
algorithm}{D.3 CNN model with Adam algorithm}}\label{d.3-cnn-model-with-adam-algorithm}}
\addcontentsline{toc}{subsubsection}{D.3 CNN model with \emph{Adam}
algorithm}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Used libraries }
\KeywordTok{library}\NormalTok{(tidyverse)}
\KeywordTok{library}\NormalTok{(tensorflow)}
\KeywordTok{library}\NormalTok{(keras)}

\CommentTok{# Define optimization algorithm to be used}
\NormalTok{adam_own =}\StringTok{ }\KeywordTok{optimizer_adam}\NormalTok{(}
    \DataTypeTok{lr =} \FloatTok{1e-1}\NormalTok{, }\CommentTok{# We adjust the learning rate, keeping the rest as defaults}
    \DataTypeTok{beta_1 =} \FloatTok{0.9}\NormalTok{, }
    \DataTypeTok{beta_2 =} \FloatTok{0.999}\NormalTok{,}
    \DataTypeTok{epsilon =} \OtherTok{NULL}\NormalTok{, }
    \DataTypeTok{decay =} \DecValTok{0}\NormalTok{, }
    \DataTypeTok{amsgrad =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{clipnorm =} \DecValTok{6}\NormalTok{, }\CommentTok{# We limit as well the max value for weights}
    \DataTypeTok{clipvalue =} \OtherTok{NULL}
\NormalTok{)}

\CommentTok{# Set hyperparameters }
\CommentTok{## The number of epochs is a hyperparameter that defines the number times }
\CommentTok{## that the learning algorithm will work through the entire training }
\CommentTok{## dataset.}
\NormalTok{epoch =}\StringTok{ }\DecValTok{50}
\CommentTok{## The batch size is a hyperparameter that defines the number of samples }
\CommentTok{## to work through before updating the internal model parameters.}
\NormalTok{batch =}\StringTok{ }\DecValTok{16000}

\CommentTok{# Limit softmax weights }
\CommentTok{## (keras uses dense layer transformation inside softmax layer by default)}
\NormalTok{softmax_weights =}\StringTok{ }\KeywordTok{list}\NormalTok{(}
    \KeywordTok{matrix}\NormalTok{(}
        \KeywordTok{c}\NormalTok{(  }\DecValTok{1}\NormalTok{,  }\DecValTok{0}\NormalTok{,  }\DecValTok{0}\NormalTok{,}
            \DecValTok{0}\NormalTok{,  }\DecValTok{1}\NormalTok{,  }\DecValTok{0}\NormalTok{,}
            \DecValTok{0}\NormalTok{,  }\DecValTok{0}\NormalTok{,  }\DecValTok{1}\NormalTok{), }
        \DataTypeTok{nrow =} \DecValTok{3}
\NormalTok{    )}
\NormalTok{)}

\CommentTok{# Setup CNN model}
\NormalTok{model_cnn =}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\CommentTok{# We reshape the dataset, as 1D convolution requires 3D tensor as input}
\StringTok{    }\KeywordTok{layer_reshape}\NormalTok{(}
        \DataTypeTok{target_shape =} \KeywordTok{c}\NormalTok{(}\DecValTok{27}\NormalTok{, }\DecValTok{1}\NormalTok{),}
        \DataTypeTok{input_shape =} \DecValTok{27}\NormalTok{,}
        \DataTypeTok{trainable =} \OtherTok{FALSE}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\CommentTok{# 1D convolution layer}
\StringTok{    }\KeywordTok{layer_conv_1d}\NormalTok{(}
        \DataTypeTok{filters =}\NormalTok{ 1L, }\CommentTok{# Dimentions of the output space}
        \DataTypeTok{kernel_size =}\NormalTok{ 9L, }\CommentTok{# Number of parameters}
        \DataTypeTok{strides =}\NormalTok{ 9L, }\CommentTok{# Strides of convolution equal to parameters side}
        \CommentTok{# The starting value is 0 to ensure reproducibility}
        \DataTypeTok{kernel_initializer =} \StringTok{"zeros"}\NormalTok{, }
        \CommentTok{# The constant is not added, because we already have "Buy" dummy}
        \DataTypeTok{use_bias =} \OtherTok{FALSE}\NormalTok{, }
        \CommentTok{# We want a linear activation function }
        \DataTypeTok{activation =} \StringTok{"linear"}\NormalTok{, }
        \DataTypeTok{input_shape =} \KeywordTok{c}\NormalTok{(}\DecValTok{27}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\CommentTok{# An inverse transformation into a 2D tensor for softmax implementation}
\StringTok{    }\KeywordTok{layer_flatten}\NormalTok{(}
        \DataTypeTok{data_format =} \StringTok{"channels_first"}
\NormalTok{    ) }\OperatorTok{%>%}
\StringTok{    }\CommentTok{# Softmax layer}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}
        \DataTypeTok{units =} \DecValTok{3}\NormalTok{, }\CommentTok{# Number of units equal to categories (3 utilities)}
        \DataTypeTok{use_bias =} \OtherTok{FALSE}\NormalTok{, }\CommentTok{# The bias constant is not estimated}
        \DataTypeTok{weights =}\NormalTok{ softmax_weights,}
        \DataTypeTok{trainable =} \OtherTok{FALSE}\NormalTok{, }\CommentTok{# This layer is fixed}
        \DataTypeTok{activation =} \StringTok{"softmax"} \CommentTok{# Softmax layer (to obtain probabilities)}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\CommentTok{# Learning algorith definition}
\StringTok{    }\KeywordTok{compile}\NormalTok{(}
        \DataTypeTok{loss =} \StringTok{"categorical_crossentropy"}\NormalTok{, }\CommentTok{# Choice of loss function}
        \DataTypeTok{optimizer =}\NormalTok{ adam_own, }\CommentTok{# Parametrised Adam}
        \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{) }\CommentTok{# Target metrics}
\NormalTok{    ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\CommentTok{# Training the model}
\StringTok{    }\KeywordTok{fit}\NormalTok{(}
\NormalTok{        X_train, Y_train, }\CommentTok{# To train the model we use 80% of our dataset}
        \DataTypeTok{epochs =}\NormalTok{ epoch,}
        \DataTypeTok{batch_size =}\NormalTok{ batch, }
        \DataTypeTok{validation_data =} \KeywordTok{list}\NormalTok{(X_test, Y_test) }\CommentTok{# 20% for validation}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\FloatBarrier

\newpage

\hypertarget{annexes}{%
\section*{Annexes}\label{annexes}}
\addcontentsline{toc}{section}{Annexes}

\hypertarget{i-simulation-tool-for-performance-comparison-of-discrete-choice-models}{%
\subsection*{I Simulation tool for performance comparison of discrete
choice
models}\label{i-simulation-tool-for-performance-comparison-of-discrete-choice-models}}
\addcontentsline{toc}{subsection}{I Simulation tool for performance
comparison of discrete choice models}

\textbf{Author:} Amirreza Talebijamalabad, M1 SIE (Grenoble INP)

\textbf{Under supervision of:} IragaÃ«l Joly, HDR (GAEL, UGA, Grenoble
INP)

\textbf{Available at:} \url{https://github.com/Amirreza-96/sdcm}

\FloatBarrier

An experimental design is a plan which identifies the independent,
dependent, and nuisance variables and indicates the way in which the
randomization and statistical aspects of an experiment are to be carried
out. Speaking of experimental design, we need to bear randomization,
replication and blocking in our minds as three key elements of the
experimental design ({\textbf{???}}). Randomization as a rather new
concept in design of experiments, plays a pivotal role in distribution
of idiosyncratic characteristics and variables' levels so that they do
not selectively bias the outcome of the experiment. For example, in our
designs, we applied randomization to avoid dominant alternatives as much
as we can. Replication is the observation of two or more experimental
units under the same conditions. Replication enables us to validate the
proposed model and ensures the precise effects. Usually, in simulation,
we run a very long replication or we make relatively many replications
but small in dimensions, which we choose to replicate once but large
enough. Blocking, on the other hand, is an experimental procedure for
isolating variation attributable to a nuisance variable. Also, making
blocks, we can randomly assign respondents to the choice sets or control
the number of respondents in order to intimate the real scenarios;
However, blocking is not of great importance when we are talking in the
realm of simulation since we can control variations and variables.

Stated choice experiments present sampled respondents with a number of
different choice situations, each consisting of a universal but finite
set of alternatives defined on a number of attribute dimensions.
Respondents are then asked to specify their preferred alternatives given
a specific hypothetical choice context. In simulation, since the
respondents are artificial, it will not be wiseful to sample the
population, instead, replicationg the processes would be fruitful as the
population will be generated repeatedly which is more close to reality.
Moreover, to simulate the choice making process, based on decision rules
such as utility maximization, utilities are calculated to reveal the
choices of the individuals. SC data requires that the analyst designs
the experiment in advance by assigning attribute levels to the
attributes that define each of the alternatives which respondents are
asked to consider({\textbf{???}}).

To generate experimental designs for SC studies we need to find out how
to allocate the attribute levels to the design matrix. Traditionally,
researchers have relied on the principle of orthogonality to populate
the choice situations shown to respondents. The orthogonality of an
experimental design relates to the correlation structure between the
attributes of the design. however, this class of designs may not be
statistically efficient, as they do not take the SC model specification
into account. These models are optimal for the linear models and assure
the researcher that multicollinearity does not exist in design.
Considering this, it is assumed that such designs can be used for the
non-linear models by linear arrangements({\textbf{???}}). It is
important to note however, that the orthogonality of a design suggests
nothing about whether two or more attributes are cognitively correlated
in the minds of the respondents (e.g.~price and quality attributes). As
such, orthogonality is purely a statistical property of the design and
not a behavioural property imposed upon the experiment({\textbf{???}}).
Moreover, by entreing non-design attributes such as socio-demographic
variables, any covariate within the dataset will unlikely be orthogonal,
not only amongst themselves, but also with the design attributes. For
example, if age, gender and income are added as variables in an
analysis, correlations are not only likely to exist for these variables,
but given that the variables described are constant over all choice
situations within individual respondents, correlations between these
variables and other attributes of the design are also likely to exist.
Simulation tool should allow us to enter or not such soci-demographic
variables to the simulation process so that at least we have some
control on correlations. Furthermore, more advanced data generation
methods should be applied to generate correlated data with specific
precision. In this research, we have made a very conventional and
widespread design so called full factorial design. It contains all of
the possible levels of factors, and allows us to estimate all of the
main effects and two-way interactions. Main effects are independent of
the levels of other attributes, however; interactions involve two or
more factors in which, effect of one factor depends on the level of
another({\textbf{???}}). Furthermore, there are fractional orthogonal
designs known as efficient designs providing ratherly small but
efficient designs, also, there are algorithms to determine the
correlations between columns as orthogonality is violated in these
designs. It would be excellent if various kinds of designs were
available in simulator.

({\textbf{???}}) conducted an empirical work to figure out consumers'
willingness to pay, and a price premium for two environmental attributes
of a non-food agricultural product(Roses). In this research there are
two unlabelled alternatives Rose A and B and one no choice alternative.
The two attributes, Label and Carbon, have two levels which make four
combinations, hence six pairs of alternatives can be drawn from these
combinations. Price ranges from 1.5 to 4.5 and is randomly assigned to
the combinations of two other attributes. Finally, each respondent is
faced with twelve choice sets(24 combinations of all attributes or 12
questions), hence, considerring no choice mode, there are three
alternatives in each question. Trying to simulate the paper's results,
we made a design with the same attributes and attribute levels. We
sample put alternatives two by two in choice sets (16 choice
sittuation), hence, respondents are faced twice with six pairs of
alternatives, but the price is randomly assigned to each of the choice
sets. Moreover, as no-choice mode does not effect the design, we do not
add this mode to the design but finally, when it comes to utility
comparison and decision process, this alternative is taken into account.
Furthermore, we have not put interaction variable in the design since as
no-choice mode, it does not affect the combinations of the design. To
add, it makes the tool more flexible if we allow the user to decide
about these two options.

({\textbf{???}}) considered four socioeconomic characteristics as well
as sex, age, income and organic purchase habit. Since no information
were available in regard to these features' correlation, we assumed that
they are uncorrelated, and made each feature independently. This is a
limitation for data generation process. Simulator must enable the user
to specify whether data is correlated or not. Moreover, it should allow
the user to enter the inputs and specifications as well as distributions
and their parameters. In order to generate sex data, we draw samples out
of a uniform distribution with parameters \(a=0,b=1\), then we assume
that there is a \(0.49\) chance that a respondent is female. Hence, if
the random number is in range \((0,0.49)\), hypothetical individual is
female, otherwise, is male. The same procedure applies to the habit
feautre. If the random number is among \((0,0.35)\), organic habit is
assumed to be zero. In order to generate age feature, the best
distribution that we can draw samples which exactly could resemble the
real data is truncated normal distribution. However, we just have the
tnormal distribution's parameters and we need to have the underlying
normal distribution's parameters(mean, and std.) to be able to draw
samples. To tackle this, we solve a system of non-linear equations
utilising numerical methods(Newton Raphson method) to find the
underlying normal dist. parameters. Another way to generate such data is
to draw samples from a log normal distribution. We still have a problem
with this way since we need to have data ranging from 18 to 85,
nevertheless positive values are generated. And finally, we simply take
draws from normal distribution with the same parameters. As future
improvements, simulation tool should be able to generate data based on
theoretical distributions or empirical ones. Hence, some curve fitting
procedures to find the distribution best fitting the real datashould be
installed in the tool.

So far, we have made the design and socioeconomic features for
artificial individuals. Now, we need to specify utilities per each
individual, and finally, due to the RUM model, we select the alternative
with highest utility per each individual per each choice set. As a
future improvement, we suggust that simulation tool should be able to
simulate decision making process based on different approaches for
example, regret minimization. In order to calculate utilities, we took
parameters from the paper (\emph{a priori}). All of the terms mentioned
in the paper including ASC are used. We take no-choice as reference
alternative. Firstly, a matrix of \(4\times 1000\) is constructed for
socioeconomic characteristics parameters. Each column indicates a person
, and all of the columns are similar since the parameters are constant
for all of the people. Then, this matrix is pointly multiplied with the
matrix of socioeconomic charachteristics matrix, and finally the sum of
each column is the socioeconomic utility of each person and a vector of
utility is achieved. Secondly, a matrix of \(1000\times 5\) is made to
contain the parameters corresponding to the alternatives(price, label,
carbon, label-carbon, constant). each row of this matrix is drawn from
multivariate normal distribution, \(\mu + L\times R\) where \(\mu\) is a
vector of means of parameters, L is derived from Cholesky
decomposition(\(L\times L^{\prime} = \sigma^2\)) and \(R\) is a vector
of \(K\) draws from a \(N~(0,1)\). Finally, this matrix is multiplied by
the inverse of design matrix which results in a matrix of
\(1000\times 36\) in which each element shows the utility of an
alternative for an individual. Consequently, we add up socioeconomic
utility to each of the columns of this matrix. This makes the observed
utility. In regard to unobserved utility, a matrix of \(1000\times 36\)
is containing the draws of \(Gumbel(0,1)\), then we add this matrix to
the previous one and this brings about the utility matrix. For the
choice selection process, columns of utility matrix are compared pair by
pair and also the max of these each of these paires is compared with an
element of \(Gumbel(0,1)\) to specify whether the individual buys or
not. Finally, we suggust that tool decode and clean the data. One
important issue is the difference between real data and simulated data
which arises from ommited variables. For example, when it comes to
reality, time is a very important factor affecting the choices made by
respondents, but when it comes to simulation, time is meaningless for
artificial individuals. These issues also need to be taken into account
specially when we are comparing estimation results of these two types of
data.

\newpage

\hypertarget{ii-reproducible-research}{%
\subsection*{II Reproducible research}\label{ii-reproducible-research}}
\addcontentsline{toc}{subsection}{II Reproducible research}

This work was accomplished with implementation of the most advanced
reproducible research techniques. First of all, a version control system
(\emph{git}) was used to track the changes and modification in the
working tree from the start of the internship. The collaboration with
other participants was organised through \emph{GitHub}, where a common
repository was maintained to store the data and document, as well as to
keep every element of the code or text available to everyone. The report
generation was automated with the use of a simplified markup language
with embedded executable \emph{R} code. For heavy tasks, such as data
generation, model estimation or big data exploration separate source
code files were used.

This short documents aims to introduce the reader to used research
methodology, that was used in this work and during the internship. The
used tool-set will be introduced.

\emph{Git} is one of the version control tools alongside SVN and
Mercurial-SCM, which allows to easily control changes and modifications
within text documents. Unfortunately the proposed functionality does not
function with more complex proprietary formats such as Word or image
based documents, such as PDF. Consequently, this tool is not practical
only for working with simple text documents: it remains absolutely
impractical for working with typical office tasks. Several text editors
for developers among which RStudio, VSCode, Atom and many other provide
possibilities to integrate \emph{git} functionality directly into the
editor and drastically optimise the workflow. This makes interacting
with \emph{git} much more comfortable than through the command line or a
standalone \emph{git} client.

\emph{GitHub} is an open source cooperative platform for developers
offered by Microsoft making it easier to work with the \emph{git}
version control service. The platform has an entire ecosystem of
extensions, expanding git functionality, as well as a set of project
management and communication tools. In total this platform offers:

\begin{itemize}
\tightlist
\item
  A cloud space to host the working files and publish the results;
\item
  A web interface to interact with \emph{git} from browser or through a
  standalone app;
\item
  A platform facilitating collaboration with other users, which
  gradually approaches in the functionality to a social network;
\item
  An integrated project management system.
\end{itemize}

To write the scientific report it was decided to use the \emph{LaTeX}
complete markup language. There are several distributions of LaTeX, one
of the verified versions to integrate well with \emph{R} being
\emph{tinytex} (which is available as \emph{tinytex} package in CRAN
repositories). However, even if \emph{LaTeX} produces well structured
documents that are easy to manage, there exists the problem of its
complexity in extending its functionalities. Consequently, it was
decided to use an intermediary simplified markup language, which is easy
to use and does not require advanced knowledge of \emph{LaTeX}:
Markdown. It allows to write documents with simple syntax, which could
be later transformed into PDF, HTML and Word documents using the
\emph{pandoc} converter.

Finally, to embed the \emph{R} code inside the document to automatically
generate the figures and tables, we used \emph{RMarkdown}, which is an
extension for \emph{Markdown} integrating \emph{R} language inside. Such
set-up ensured, that the documents will be easy to share and modify,
preserving at the same time all their functionality. This work offers
all the necessary elements to be fully reproducible.

\textbf{The resulting compedium is available at:}
\url{https://github.com/nikitagusarov/performance_exploration}

\newpage


\end{document}
