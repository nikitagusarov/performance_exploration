# Evaluating Learning Algorithms: A Classification Perspective

## Authors

Nathalie Japkowicz, Ottawa Mohah Shah

## Purpose 

The purpose of this book is to bring both researchers and practitioners to a levelof sophistication in statistical analysis and choices of evaluation methods andmetrics sufficient to conduct proper evaluation of algorithms. 
To date, aside froma small group of scholars, data mining practitioners as well as researchers havenot been questioning the way they conduct the evaluation of their classifiers tothe extent and with the rigor that this issue warrants. 

A look at several recently published studies points to a numberof important issues that should be put on the table when evaluating algorithms. 
Criticisms go as far as claiming that the improvements observed by our currentevaluation methods are in fact much less impressive than they appear (Hand,2006).

The  view  of  evaluation  presented  in  this  book  is  not  necessarily  shared  byeveryone in the community. 
Two notable voices in apparent disagreement arethose of Chris Drummond and Janez Demsar (Drummond, 2006; Demsar,2008; Drummond, 2008). 
Favoring a less-rigorous approach to evaluation, theyargue that it is not the machine learning researcher’s job to perform such strictevaluation. 
Rather,  the  researcher  should  follow  an  exploratory  approach  tomachine learning and have recourse to only light methods of evaluation, mainlyto guide the research. 

## Introduction

There aremany issues involved in the question of designing  an evaluation strategy fora learning machine.

Forinstance, one may ask the following questions: 

- What precise measure is bestsuited for a quantified assessments of different algorithms’ property of interest ina given domain? 
- How can these measures be efficiently computed? 
- Do the datafrom the domain of interest affect the efficiency of this calculation? 
- How canwe be confident about whether the difference in measurement for two or morealgorithms denotes a statistically significant difference in their performance? 
- Is this statistical difference practically relevant as well? 
- How can we best use theavailable data to discover whether such differences exist?

In particular, we must understandthe strengths and limitations of these approaches as well as the proper mannerin which they should be applied. 
Moreover, we also need to understand whatthese methods offer and how to properly interpret the results of their application. 
This is very different from the way evaluation has been perceived to date in themachine learning community, where we have been using a routine, de facto,strategy, without much concern about its meaning.

In this book, we try to address these issues, more specifically with regardto  the  branch  of  machine  learning  pertaining  to  classification  algorithms.  
In particular, we focus on **evaluating the performance of classifiers generated bysupervised  learning  algorithms**,  generally  in  a  binary  classification  scenario.

## Traditional approach to performance evaluation

Kibler and Langley (1988) - suggesting the need for a greater emphasis on performance evaluation.

Witten and Frank (2005) - prescribe repeated cross-validation as a de facto method formost practical limited data situations. 

*Comparing the performance of different machine learning methods on agiven problem is another matter that is not so easy as it sounds: to be surethat apparent differences are not caused by chance effects, statistical testsare needed*

Generally the machine learning community has settled onmerely  rejecting  the  null  hypothesis  that  the  apparent  differences  are  causedby  chance  effects  when  thettest  is  applied.

Progress toward realizing the goal of more meaningful classifierevaluation  and consequently  better  understanding  of the  learning  approachesthemselves can take place only if both the researchers involved in developingnovel learning approaches and the practitioners applying these are better awareof not only the evaluation methods, but also of their strengths and limitationstogether with their context of application.

There  have  also  been  criticisms  of  specific  evaluation  methods  that  werecondemned for not yielding the desired results. 
These criticisms, in fact, arisefrom unreasonable expectations from the evaluation approaches. 
It is importantto understand what a given evaluation method promises and how the results itobtained should be interpreted.

The approaches utilized to do so proceed along the following lines, withsome minor variations:  

1. Select an evaluation metric, the most often used onebeing accuracy; 
2. Select a large-enough number of datasets [the number is chosenso as to be able to make a convincing case of apt evaluation and the datasets aregenerally obtained from a public data repository, *the main one being the University of California, Irvine, (UCI) machine learning repository*]; 
3. Select the best parameters for various learning algorithms, a task generally known as modelselection  but  mostly  inadvertently  interleaved  with  evaluation;  
4. Use a k-foldcross-validation technique for error estimation, often stratified 10-fold cross-validation, with or without repetition; 
5. Apply pairedttests to all pairs of resultsor to the pairs deemed relevant (e.g., the ones including a possibly new algo-rithm of interest) to test for statistical significance in the observed performancedifference; 
6. Average the results for an overall estimate of the algorithm’s perfor-mance or, alternatively, record basic statistics such as win/loss/ties for each algo-rithm with respect to the others.

## Problems of traditional approach

The main problems may be classified as :

1. Statistical validity
2. Evaluation Metric
3. Aggregating the Results
4. Dataset Selection 
5. Model selection versus Evaluation
6. Internal versus External Validation

Here is more detailed discussion :

- Sample size of the domains (k-fold crossvalidation requires a significant sample size). 
- The inter-dependence between the number of experiments and the significance level of astatistical test.
- Regarding the use of “*accuracy*” as an evaluation metric, it should be noted that a number of domains used may be imbalanced.
- The win/tie/loss results give us quantitativebut not qualitative assessments: we know how many times each algorithm wonover, tied with, or lost against each other, but not by how much.
- There  have  been  a  number  of  criticisms  related  to  thedatasets on which machine learning experiments are undertaken.
- An additional problem with the de factoapproach  to  evaluation  is  the  fact  that  the  purpose  of  the  evaluation  proce-dure is rarely clarified. Evaluation could be done with the purpose of selectingappropriate  parameters  for  the  different  classifiers  considered  (model  selection) or selecting one or several classifiers from among a number of classifiers available. This exploratory pursuit, though, is different from the issue of decid-ing what classifier is best for a task or for a series of tasks (the primary purposeof evaluation in learning experiments). 
- Although we carefully use cross-validation to train andtest on different partitions of the data, at the end of the road, these partitions allcome from the same distribution because they belong to the same dataset. Yetthe designed classifiers are applied to different, even if related, data.

The main questions and problems treated in te handbook

- Do we measure how accurate the algorithm is? If so, how do we define accuracy ? 
- Do we value one aspect of the algorithm’s performance more than other ? 
- How to estimate it in as unbiased a manneras possible, making the best possible use of the available data ?
- Whether the differences in the performances obtained by the algorithm alone orin relation to others are statistically significant ?
- What domains can be deemed suitable as benchmarks to evaluate learning approaches ?
- As a consequence of the inherent multidisciplinary nature of themachine learning tasks, different variants of these performance measures have been influenced by approaches from a variety of disciplines, including statistics, medicine, and information retrieval. 

## Other issues to take into account

One of the main cautions to be exerted in tuning an algo-rithm, i.e., choosing the best learning parameters, alternatively known as modelselection, is that such parameter selection should be performedindependently ofthe test set. 
This, however, is seldom the case with the situation being aggravatedin the case of repeated experimental runs. 
Unfortunately, the adjustments to thestatistical tests necessary in such a situation are usually not made.

It is important to know whether wewill be comparing a new algorithm with one or several existing ones; whetherthis comparison will take place in one domain of interest or more; whether weare looking for specific properties of the algorithms we are testing or generalones; and so on. In the absence of such goal-oriented definitions, we may be ableto generate results showing advantages or disadvantages of different methods,but it is unclear how these results relate to reality.

Exploratory pursuit versus Final Evaluation problem.
The division between the twokinds of research is problematic. 
Although it is clear that they should be sepa-rated, some researchers (e.g.,Drummond, 2006) recognize that they cannot be,and that, as a result, we should not even get into issues of statistical validationof our results and so on, because we are doing only exploratory research thatdoes not demand such formal testing. 
This is a valid point of view, although wecannot help but hope that the results we obtain be more definitive than thoseadvocated by this point of view.

## A point on artificial datasets

Although there is unquestionable value inreal data, this does not diminish the unique advantages that might be obtainedfrom using artificial data to obtain insights into the behavior of the algorithms. 
Real data are good at informing us about different aspects of the world, someof which we may have overlooked. 
Artificial data, on the other hand, allow usto explore variabilities not found in the real data we have collected that yet canreasonably  be  expected  in  practice. 
Artificial  data  can  also  be  designed  in  acontrolled manner to study specific aspects of the performance of algorithms.
Consequently such data may allow for tighter control, which gives rise to morecarefully constructed and more enlightening experiments. 
Although, on the onehand, real data are hard to come by, on the other hand, artificial data present the danger of oversimplifying the problem. 
The debate between limited but realdatasets versus artificial data is likely to continue. 
However, an argument can bemade to utilize the two in tandem for better evaluation.

## A discussion on the evaluation

Is Evaluation an End in Itself? 
Certainly not. 
Evaluation approaches should not be perceived as ends in them-selves. 
Our ultimate goal is the pursuit of learning algorithms that can approachthe behavior of the domains that they model. 
Evaluation methods should be astep further in the direction of developing an understanding of the approachesat hand, along with their strengths and limitations in the domain in which they are applied.

The evaluation studies should provide us with feedback onhow the algorithmic theories should be refined or, possibly, how innovationsshould be made on them. 
It  is  important  to  note  that  evaluation  approaches  are  our  currently  bestavailable tools that enable us to notice the anomalies in algorithmic behaviorin addition to studying their positive characteristics.

## Performance measures

The evaluation of learning algorithms both in absolute terms and in relation toother algorithms involves addressing four main components :

- performance measures,
- error estimation,
- statistical significance testing, 
- test benchmark selection







## Addition to the litterature to seek

- D. J. Hand. Classifier technology and the illusion of progress.Statistical Science, 21:1–15,2006.
- D. J. Hand. Measuring classifier performance: A coherent alternative to the area under the ROC curve.Machine Learning, 77:103–123, 2009.
- C. Drummond. Machine  learning  as  an  experimental  science  (revised). In Proceedings of the AAAI’06 Workshop on Evaluation Methods for Machine Learning I. American Association for Artificial Intelligence, Menlo Park, CA, 2006.
- C. Drummond. Finding a balance between anarchy and orthodoxy. In Proceedings of the ICML’08 Third Workshop on Evaluation Methods for Machine Learning. Association for Computing Machinery, New York, 2008.
- C. Drummond and N. Japkowicz. Warning: Statistical benchmarking is addictive. Kick-ing the habit in machine learning.Journal of Experimental and Theoretical ArtificialIntelligence, 22(1):67–80, 2010