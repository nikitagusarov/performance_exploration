# Statistical methods versus neural networks in transportation research: Differences, similarities and some insights

## Authors

M.G. Karlaftis, E.I. Vlahogianni

## Abstract

In the field of transportation, data analysis is probably the most important and widely used research tool available. 
In the data analysis universe, there are two ‘*schools of thought*’; the first uses statistics as the tool of choice, while the second – one of the many methods from – Computational Intelligence. 
Although the goal of both approaches is the same, the two have kept each other at arm’s length. Researchers frequently fail to communicate and even understand each other’s work. 
In this paper, we discuss differences and similarities between these two approaches, we review relevant literature and attempt to provide a set of insights for selecting the appropriate approach.

## Keywords

Statistical models, Neural networks, Transportation research

## Introduction

Transportation data are most commonly modeled using two different approaches: statistics and/or Computational Intelligence (CI). 
The first, statistics, is the mathematics of collecting, organizing and interpreting numerical data, particularly when these data concern the analysis of population characteristics by inference from sampling (Glymour et al., 1997). 
Statistics have solid and widely accepted mathematical foundations and can provide insights on the mechanisms creating the data. 
However, they frequently fail when dealing with complex and highly nonlinear data (curse of dimensionality). 
The second, CI, combines elements of learning, adaptation, evolution and fuzzy logic to create models that are ‘‘intelligent’’ in the sense that structure emerges from an unstructured beginning (the data) (Engelbrecht, 2007; Sadek et al., 2003).
Neural Networks (NN), an extremely popular class of CI models, has been widely applied to various transportation problems, partly because they are very generic, accurate and convenient mathematical models able to easily simulate numerical model components. 
They have the inherent propensity for storing empirical knowledge and can be used in any of three basic manners (Haykin, 1999): 

1. As models of biological nervous systems. 
2. As real-time adaptive signal processors/controllers.
3. As data analytic methods. 

In transportation research, NN have been mainly used as data analytic methods because of their ability to work with massive amounts of multi-dimensional data, their modeling flexibility, their learning and generalization ability, their adaptability and their – generally – good predictive ability.

## Differences

*Terminology used by each discipline*. 
Most terms used in NN modeling are entirely different from those in statistics; Sarle (1994) made an effort to codify neural network terminology in statistical terms (or statistical terminology in NN terms!). 
Still, however, there is significant ambiguity when it comes to the terminology used that may be attributed to NN applications whose phrasing has lacked continuity on several occasions; this has led to a confusion for a number of researchers, who fail to understand the – clear in our opinion – correspondence between the two approaches and has been a source of criticism and uncertainty between transportation researchers.

*NN emphasize implementation while statistics largely emphasize inference and estimation* (Hand, 2000). 
As Breiman (2001) discusses, statistics treat data as having been generated from a stochastic process, whereas NN assume data to have been generated by a mechanism of unknown dynamics. 
In general, the process is the same regardless of the approach used; that is, to recognize and define the problem, select a method to solve it, and, then, interpret the results (Wild and Pfannkuch, 1999).

*The goals of each approach*. 
Statistics aim at providing a model – a predictor or a classifier for example – and offering insights on the data and its structure; the elements of such a model should be self-explanatory (Wild and Pfannkuch, 1999). 
Further, statistics explain the phenomena investigated by interpreting marginal effects and signs, studying elasticities and estimator properties. 
On the other hand, most NN applications do not target interpretation, but rather aim at providing an efficient – in terms of accuracy and development time – representation of the underlying properties of the data and offer good predictions for the phenomenon under study.

*The core model development process for each approach, viewed through four steps: learning, definition and interpretation, assumptions, and collinearity*. 
A fundamental difference between statistics and NN is the learning process in NN which, regardless of the method used (supervised or unsupervised, maximum likelihood or Bayesian, and so on), results in more than one model; 
this is in stark contrast with (classical) statistics which result in one final model. 
In NN, the learning curve may have various local minima and a NN model may converge to various sequential architectures that are not necessarily nested (Ripley, 1996); 
this inherent characteristic makes NN modeling more flexible than statistics, since the functional form is approximated via learning and not a priori assumed as is done in statistics (Warner and Misra, 1996).

*Parameter definition and interpretation*. 
Statistical methods are generally defined in terms of the mathematical model they use and of the statistical properties of their results, whereas NN are often defined in terms of their architecture and their learning algorithms. 
Researchers implementing NN models, in contrast to statisticians who spend more time analyzing the problem and controlling the validity of their hypotheses, assume that the model will learn the desired relations in the data without intervention or inclusion of a priori knowledge (Flexer, 1996).

*The assumptions/limitations of the two approaches*. 
Statistics frequently make a number of hypotheses and place restrictions on developing models that are not made in NN. 
For example, regression cannot deal effectively with nonlinearity while NN are inherently nonlinear nonparametric models that can straightforwardly deal with indefinable nonlinearity (DeTienne et al., 2003). 
Further, models in statistics are specified a priori and strict hypotheses are made regarding the error term; on the other hand, NN parameters are extremely adaptable, few – if any – assumptions are made regarding the error term and the construction of complex (regression-like) models is feasible without prior model or error distribution specifications (Hanson, 1995).

*The knowledge acquisition process for each approach*. 
Statistics are, in the minds of students (frequently justified), a tough discipline, while most times students do not see the need to be involved in the modeling of large datasets (Nicholls, 1999). 
On the other hand, new NN software packages have simplified the development of extremely sophisticated NN models, while the corresponding statistical models would be almost impossible to develop (Kuan and White, 1994).