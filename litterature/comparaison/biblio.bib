##################################################
# Model comparison mthods, assessing performance #
##################################################

@article{karlafti2011svsml,
    title = "Statistical methods versus neural networks in transportation research: Differences, similarities and some insights",
    journal = "Transportation Research Part C: Emerging Technologies",
    volume = "19",
    number = "3",
    pages = "387 - 399",
    year = "2011",
    issn = "0968-090X",
    doi = "https://doi.org/10.1016/j.trc.2010.10.004",
    url = "http://www.sciencedirect.com/science/article/pii/S0968090X10001610",
    author = "M.G. Karlaftis and E.I. Vlahogianni",
    keywords = "Statistical models, Neural networks, Transportation research",
    abstract = "In the field of transportation, data analysis is probably the most important and widely used research tool available. In the data analysis universe, there are two ; the first uses statistics as the tool of choice, while the second Computational Intelligence. Although the goal of both approaches is the same, the two have kept each other at arms work. In this paper, we discuss differences and similarities between these two approaches, we review relevant literature and attempt to provide a set of insights for selecting the appropriate approach."
}

@article{askin2013cp,
    title = "Comparing the Predictive and Classification Performances of Logistic Regression and Neural Networks: A Case Study on Timss 2011",
    journal = "Procedia - Social and Behavioral Sciences",
    volume = "106",
    pages = "667 - 676",
    year = "2013",
    note = "4th International Conference on New Horizons in Education",
    issn = "1877-0428",
    doi = "https://doi.org/10.1016/j.sbspro.2013.12.076",
    url = "http://www.sciencedirect.com/science/article/pii/S1877042813046910",
    author = "Oykum Esra Askin and Fulya Gokalp",
    keywords = "TIMSS 2011, neural networks, logistic regression",
    abstract = "Investigating effective factors on students’ achievement has wide application area in educational studies. Specially, Trends in International Mathematics and Science Study (TIMSS) allows researchers to determine correlates of mathematics and science achievement for different countries. In this study, the predictive and classification performances of logistic regression and neural networks are compared to identify the impact levels of variables on students’ mathematics achievement in Turkey. Age, gender and scales created by TIMSS team for 8th grade students (students like learning, value learning, confident in math, engaged in math, bullied at school, home educational resources), are selected as predictive variables. Model fitting statistics show that two methods give similar results in prediction and classification. In addition to model results, students’ confidence is found as the most effective factor to improve mathematics achievement."
}

@article{liu2011lr,
    author={Liu, Yuan Y.; Yang, Min; Ramsay, Malcolm; Li, Xiao S.; Coid, Jeremy W.},
    year={2011},
    date={2011/12/01},
    title={A Comparison of Logistic Regression, Classification and Regression Tree, and Neural Networks Models in Predicting Violent Re-Offending},
    journal={Journal of Quantitative Criminology},
    pages={547-573},
    volume={27},
    issue={4},
    abstract={Previous studies that have compared logistic regression (LR), classification and regression tree (CART), and neural networks (NNs) models for their predictive validity have shown inconsistent results in demonstrating superiority of any one model. The three models were tested in a prospective sample of 1225 UK male prisoners followed up for a mean of 3.31 years after release. Items in a widely-used risk assessment instrument (the Historical, Clinical, Risk Management-20, or HCR-20) were used as predictors and violent reconvictions as outcome. Multi-validation procedure was used to reduce sampling error in reporting the predictive accuracy. The low base rate was controlled by using different measures in the three models to minimize prediction error and achieve a more balanced classification. Overall accuracy of the three models varied between 0.59 and 0.67, with an overall AUC range of 0.65–0.72. Although the performance of NNs was slightly better than that of LR and CART models, it did not demonstrate a significant improvement.},
    ibsn={1573-7799},
    url={https://doi.org/10.1007/s10940-011-9137-7},
    doi={10.1007/s10940-011-9137-7}
}

@inproceedings{schulz2014predict,
    title={Predict choice: A comparison of 21 mathematical models},
    author={Schulz, Eric and Speekenbrink, Maarten and Shanks, David R},
    booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
    volume={36},
    number={36},
    year={2014}
}

@article{baogan2008ec,
    title = "Evaluation Criteria Based on Mutual Information for Classifications Including Rejected Class",
    journal = "Acta Automatica Sinica",
    volume = "34",
    number = "11",
    pages = "1396 - 1403",
    year = "2008",
    issn = "1874-1029",
    doi = "https://doi.org/10.1016/S1874-1029(08)60061-0",
    url = "http://www.sciencedirect.com/science/article/pii/S1874102908600610",
    author = "Bao-Gang HU and Yong WANG",
    keywords = "Entropy, mutual information, evaluation criteria, classification, confusion matrix, machine learning",
    abstract = "Different from the conventional evaluation criteria using performance measures, information theory based criteria present a unique beneficial feature in applications of machine learning. However, we are still far from possessing an in-depth understanding of the “entropy” type criteria, say, in relation to the conventional performance-based criteria. This paper studies generic classification problems, which include a rejected, or unknown, class. We present the basic formulas and schematic diagram of classification learning based on information theory. A closed-form equation is derived between the normalized mutual information and the augmented confusion matrix for the generic classification problems. Three theorems and one set of sensitivity equations are given for studying the relations between mutual information and conventional performance indices. We also present numerical examples and several discussions related to advantages and limitations of mutual information criteria in comparison with the conventional criteria."
}

@article{sokolova2009sa,
    title = "A systematic analysis of performance measures for classification tasks",
    journal = "Information Processing & Management",
    volume = "45",
    number = "4",
    pages = "427 - 437",
    year = "2009",
    issn = "0306-4573",
    doi = "https://doi.org/10.1016/j.ipm.2009.03.002",
    url = "http://www.sciencedirect.com/science/article/pii/S0306457309000259",
    author = "Marina Sokolova and Guy Lapalme",
    keywords = "Performance evaluation, Machine Learning, Text classification",
    abstract = "This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier’s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies."
}

@article{zhu2017es,
    title = "An evaluation study on text categorization using automatically generated labeled dataset",
    journal = "Neurocomputing",
    volume = "249",
    pages = "321 - 336",
    year = "2017",
    issn = "0925-2312",
    doi = "https://doi.org/10.1016/j.neucom.2016.04.072",
    url = "http://www.sciencedirect.com/science/article/pii/S0925231217305696",
    author = "Dengya Zhu and Kok Wai Wong",
    keywords = "Text mining, Text categorization, Machine learning, Evaluation, Feature selection, Benchmark collection",
    abstract = "Naïve Bayes, k-nearest neighbors, Adaboost, support vector machines and neural networks are five among others commonly used text classifiers. Evaluation of these classifiers involves a variety of factors to be considered including benchmark used, feature selections, parameter settings of algorithms, and the measurement criteria employed. Researchers have demonstrated that some algorithms outperform others on some corpus, however, inconsistency of human labeling and high dimensionality of feature spaces are two issues to be addressed in text categorization. This paper focuses on evaluating the five commonly used text classifiers by using an automatically generated text document collection which is labeled by a group of experts to alleviate subjectivity of human category assignments, and at the same time to examine the influence of the number of features on the performance of the algorithms."
}

@article{yangho2020cpa,
    title = "Avoid Oversimplifications in Machine Learning: Going beyond the Class-Prediction Accuracy",
    journal = "Patterns",
    volume = "1",
    number = "2",
    pages = "100025",
    year = "2020",
    issn = "2666-3899",
    doi = "https://doi.org/10.1016/j.patter.2020.100025",
    url = "http://www.sciencedirect.com/science/article/pii/S2666389920300258",
    author = "Sung Yang Ho and Limsoon Wong and Wilson Wen Bin Goh",
    keywords = "machine learning, data science, validation, artificial intelligence",
    abstract = "Class-prediction accuracy provides a quick but superficial way of determining classifier performance. It does not inform on the reproducibility of the findings or whether the selected or constructed features used are meaningful and specific. Furthermore, the class-prediction accuracy oversummarizes and does not inform on how training and learning have been accomplished: two classifiers providing the same performance in one validation can disagree on many future validations. It does not provide explainability in its decision-making process and is not objective, as its value is also affected by class proportions in the validation set. Despite these issues, this does not mean we should omit the class-prediction accuracy. Instead, it needs to be enriched with accompanying evidence and tests that supplement and contextualize the reported accuracy. This additional evidence serves as augmentations and can help us perform machine learning better while avoiding naive reliance on oversimplified metrics."
}

@article{andersson1999cpe,
    title = "Measure-based classifier performance evaluation",
    journal = "Pattern Recognition Letters",
    volume = "20",
    number = "11",
    pages = "1165 - 1173",
    year = "1999",
    issn = "0167-8655",
    doi = "https://doi.org/10.1016/S0167-8655(99)00084-7",
    url = "http://www.sciencedirect.com/science/article/pii/S0167865599000847",
    author = "Arne Andersson and Paul Davidsson and Johan Lindén",
    keywords = "Classifier performance evaluation, Cross-validation, Generalization",
    abstract = "The concept of measure functions for classifier performance is suggested. This concept provides an alternative way of selecting and evaluating learned classifiers, and it allows us to define the learning problem as a computational problem."
}

@article{zhou2020mrc,
    title = "An analysis on the relationship between uncertainty and misclassification rate of classifiers",
    journal = "Information Sciences",
    volume = "535",
    pages = "16 - 27",
    year = "2020",
    issn = "0020-0255",
    doi = "https://doi.org/10.1016/j.ins.2020.05.059",
    url = "http://www.sciencedirect.com/science/article/pii/S0020025520304461",
    author = "Xinlei Zhou and Xizhao Wang and Cong Hu and Ran Wang",
    keywords = "Uncertianty, Supervised learning, Classification problem, Misclassification rate, Statistical distribution",
    abstract = "This paper provides new insight into the analysis on the relationship between uncertainty and misclassification of a classifier. We formulate the relationship explicitly by taking entropy as a measurement of uncertainty and by analyzing the misclassification rate based on the membership degree difference. Focusing on binary classification problems, this study theoretically and experimentally validates that the misclassification rate will definitely be upgrading with the increase of uncertainty if two conditions are satisfied: (1) the distributions of two classes based on membership degree difference are unimodal, and (2) these two distributions attain peaks when the membership degree difference is less and larger than zero, respectively. This work aims to provide some practical guidelines for improving classifier performance through clearly expressing and understanding the relationship between uncertainty and misclassification of a classifier."
}

@article{armano2015cba,
    title = "A direct measure of discriminant and characteristic capability for classifier building and assessment",
    journal = "Information Sciences",
    volume = "325",
    pages = "466 - 483",
    year = "2015",
    issn = "0020-0255",
    doi = "https://doi.org/10.1016/j.ins.2015.07.028",
    url = "http://www.sciencedirect.com/science/article/pii/S0020025515005241",
    author = "Giuliano Armano",
    keywords = "Classifier performance measures, Feature ranking/selection, Confusion matrices",
    abstract = "Performance measures are used in various stages of the process aimed at solving a classification problem. Unfortunately, most of these measures are in fact biased, meaning that they strictly depend on the class ratio – i.e. on the imbalance between negative and positive samples. After pointing to the source of bias for the best known measures, novel unbiased measures are defined which are able to capture the concepts of discriminant and characteristic capability. The combined use of these measures can give important information to researchers involved in machine learning or pattern recognition tasks, in particular for classifier performance assessment and feature selection."
}

@article{chen2012cvtt,
    title = "Classifier variability: Accounting for training and testing",
    journal = "Pattern Recognition",
    volume = "45",
    number = "7",
    pages = "2661 - 2671",
    year = "2012",
    issn = "0031-3203",
    doi = "https://doi.org/10.1016/j.patcog.2011.12.024",
    url = "http://www.sciencedirect.com/science/article/pii/S0031320312000180",
    author = "Weijie Chen and Brandon D. Gallas and Waleed A. Yousef",
    keywords = "Classifier evaluation, Training variability, Classifier stability, -statistics, ",
    abstract = "We categorize the statistical assessment of classifiers into three levels: assessing the classification performance and its testing variability conditional on a fixed training set, assessing the performance and its variability that accounts for both training and testing, and assessing the performance averaging over training sets and its variability that accounts for both training and testing. We derived analytical expressions for the variance of the estimated AUC and provide freely available software implemented with an efficient computation algorithm. Our approach can be applied to assess any classifier that has ordinal (continuous or discrete) outputs. Applications to simulated and real datasets are presented to illustrate our methods."
}

@article{tama2019ecct,
    title = "An empirical comparison of classification techniques for next event prediction using business process event logs",
    journal = "Expert Systems with Applications",
    volume = "129",
    pages = "233 - 245",
    year = "2019",
    issn = "0957-4174",
    doi = "https://doi.org/10.1016/j.eswa.2019.04.016",
    url = "http://www.sciencedirect.com/science/article/pii/S0957417419302465",
    author = "Bayu Adhi Tama and Marco Comuzzi",
    keywords = "Process indicators, Classification algorithms, Significance test, Performance evaluation, Event log, Empirical benchmark",
    abstract = "Predictive analytics is an essential capability in business process management to forecast future status and performance of business processes. In this paper, we focus on one particular predictive monitoring task that is solved using classification techniques, i.e. predicting the next event in a case. Several different classifiers have been recently employed in the literature in this task. However, a quantitative benchmark of different classifiers is currently lacking. In this paper, we build such a benchmark by taking into account 20 classifiers from five families, i.e. trees, Bayesian, rule-based, neural and meta classifiers. We employ six real-world process event logs and consider two different sampling approaches, i.e. case and event-based sampling, and three different validation methods in order to acquire a comprehensive evaluation about the classifiers’ performance. According to our benchmark, the classifier most likely to be the overall superior performer is the credal decision tree (C-DT), followed by the other top-4 performers, i.e. random forest, decision tree, dagging ensemble, and nested dichotomies ensemble. We also provide a qualitative discussion of how features of an event log can affect the choice of best classifier."
}

@article{freitas2014ccm,
    author = {Freitas, Alex A.},
    title = {Comprehensible Classification Models: A Position Paper},
    year = {2014},
    issue_date = {June 2013},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {15},
    number = {1},
    issn = {1931-0145},
    url = {https://doi.org/10.1145/2594473.2594475},
    doi = {10.1145/2594473.2594475},
    journal = {SIGKDD Explor. Newsl.},
    month = mar,
    pages = {1–10},
    numpages = {10},
    keywords = {nearest neighbors, Bayesian network classifiers, monotonicity constraint, decision tree, decision table, rule induction}
}

@article{portugal2018mlrs,
    title = "The use of machine learning algorithms in recommender systems: A systematic review",
    journal = "Expert Systems with Applications",
    volume = "97",
    pages = "205 - 227",
    year = "2018",
    issn = "0957-4174",
    doi = "https://doi.org/10.1016/j.eswa.2017.12.020",
    url = "http://www.sciencedirect.com/science/article/pii/S0957417417308333",
    author = "Ivens Portugal and Paulo Alencar and Donald Cowan",
    keywords = "Systematic review of the literature, Recommender systems, Machine learning, Machine learning algorithms, Application domains, Performance metrics",
    abstract = "Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics."
}

@article{hagenauer2017mlcmc,
    title = "A comparative study of machine learning classifiers for modeling travel mode choice",
    journal = "Expert Systems with Applications",
    volume = "78",
    pages = "273 - 282",
    year = "2017",
    issn = "0957-4174",
    doi = "https://doi.org/10.1016/j.eswa.2017.01.057",
    url = "http://www.sciencedirect.com/science/article/pii/S0957417417300738",
    author = "Julian Hagenauer and Marco Helbich",
    keywords = "Travel mode choice, Classification, Machine learning, The Netherlands",
    abstract = "The analysis of travel mode choice is an important task in transportation planning and policy making in order to understand and predict travel demands. While advances in machine learning have led to numerous powerful classifiers, their usefulness for modeling travel mode choice remains largely unexplored. Using extensive Dutch travel diary data from the years 2010 to 2012, enriched with variables on the built and natural environment as well as on weather conditions, this study compares the predictive performance of seven selected machine learning classifiers for travel mode choice analysis and makes recommendations for model selection. In addition, it addresses the importance of different variables and how they relate to different travel modes. The results show that random forest performs significantly better than any other of the investigated classifiers, including the commonly used multinomial logit model. While trip distance is found to be the most important variable, the importance of the other variables varies with classifiers and travel modes. The importance of the meteorological variables is highest for support vector machine, while temperature is particularly important for predicting bicycle and public transport trips. The results suggest that the analysis of variable importance with respect to the different classifiers and travel modes is essential for a better understanding and effective modeling of people’s travel behavior."
}

@article {kubus2020ec,
    author = "Mariusz Kubus",
    title = "Evaluation of Resampling Methods in the Class Unbalance Problem",
    journal = "Econometrics",
    year = "2020",
    publisher = "Sciendo",
    address = "Berlin",
    volume = "24",
    number = "1",
    pages = "39 - 50",      
    url = "https://content.sciendo.com/view/journals/eada/24/1/article-p39.xml"
}

@inproceedings{drummond2006mles,
    title={Machine learning as an experimental science (revisited)},
    author={Drummond, Chris},
    booktitle={Proceedings of the Twenty-First National Conference on Artificial Intelligence: Workshop on Evaluation Methods for Machine Learning},
    pages={1--5},
    year={2006}
}

@inproceedings{costa2007rev,
    title={A review of performance evaluation measures for hierarchical classifiers},
    author={Costa, Eduardo and Lorena, Ana and Carvalho, ACPLF and Freitas, Alex},
    booktitle={Evaluation Methods for machine Learning II: papers from the AAAI-2007 Workshop},
    pages={1--6},
    year={2007}
}

@inproceedings{flach2019peml,
    title={Performance Evaluation in Machine Learning: The Good, the Bad, the Ugly, and the Way Forward},
    author={Flach, Peter},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={33},
    pages={9808--9814},
    year={2019}
}